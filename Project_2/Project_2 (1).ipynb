{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter EMR notebook for Project Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1588701453956_0008</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-152.ec2.internal:20888/proxy/application_1588701453956_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-230.ec2.internal:8042/node/containerlogs/container_1588701453956_0008_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fcb288a1b00>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "df = spark.read\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .option(\"inferSchema\", \"true\")\\\n",
    "          .option(\"basePath\", \"hdfs:///hive/amazon-reviews-pds/parquet/\")\\\n",
    "          .parquet(\"hdfs:///hive/amazon-reviews-pds/parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date', 'year', 'product_category']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only limited number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['customer_id', 'review_id', 'product_id', 'product_parent', \n",
    "                   'product_title', 'star_rating', 'helpful_votes', 'total_votes',\n",
    "                   'verified_purchase', 'review_date', 'year', 'product_category','review_body','review_headline']\n",
    "df_limited = df.select(columns_to_keep).filter(F.col(\"year\")>2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "#print schema\n",
    "df_limited.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the Data before further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[customer_id: string, review_id: string, product_id: string, product_parent: string, product_title: string, star_rating: int, helpful_votes: int, total_votes: int, verified_purchase: string, review_date: date, year: int, product_category: string, review_body: string, review_headline: string, rownum: int]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import row_number\n",
    "temp_data = df_limited.withColumn(\"rownum\",row_number().over(Window.partitionBy(\"customer_id\",\"product_id\").orderBy(\"customer_id\",\"product_id\")))\n",
    "\n",
    "Filter_data = temp_data.rownum.isin(1)\n",
    "Filtered_data = temp_data.where(Filter_data)\n",
    "Filtered_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------+-----------------+---------------------+\n",
      "|product_category      |Average-star_rating|Sum-helpful_votes|Average-helpful_votes|\n",
      "+----------------------+-------------------+-----------------+---------------------+\n",
      "|Wireless              |3.893              |7934542          |0.879                |\n",
      "|Video_DVD             |4.336              |16816126         |2.594                |\n",
      "|PC                    |4.091              |10044848         |1.444                |\n",
      "|Mobile_Apps           |4.035              |18098912         |2.659                |\n",
      "|Digital_Video_Download|4.208              |2538073          |0.491                |\n",
      "|Digital_Ebook_Purchase|4.312              |19141273         |0.998                |\n",
      "|Books                 |4.364              |53565320         |2.994                |\n",
      "+----------------------+-------------------+-----------------+---------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, approx_count_distinct, count, sum, mean, round\n",
    "df_limited.groupBy(\"product_category\").agg(round(mean(\"star_rating\"),3).alias(\"Average-star_rating\"),\n",
    "                         round(sum(\"helpful_votes\"),3).alias(\"Sum-helpful_votes\"),\n",
    "                         round(mean(\"helpful_votes\"),3).alias(\"Average-helpful_votes\"))\\\n",
    "                     .sort(\"product_category\", ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+-----------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+\n",
      "|customer_id|     review_id|product_id|product_parent|          product_title|star_rating|helpful_votes|total_votes|verified_purchase|review_date|year|product_category|\n",
      "+-----------+--------------+----------+--------------+-----------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+\n",
      "|   16868299|R1UQX4QQ4IU00P|B005I6EU7U|      90774528|   Verso Kindle Fire...|          5|            0|          0|                Y| 2012-06-29|2012|              PC|\n",
      "|   26929322|R138SH9Y2VTYOB|B00AFY8K7G|     764843002|Ozone Rage ST ゲーミ...|          3|            0|          1|                Y| 2014-04-09|2014|              PC|\n",
      "|   42470542|R29LNYEZJ3YNT8|B007R1SIY0|     112516052|   CISCO SYSTEMS WAP...|          5|            0|          2|                N| 2012-06-29|2012|              PC|\n",
      "|   43829974|R3HL06XL1C5Z5S|B006HUMYCO|     569961969|   ASUS SDRW-08D2S-U...|          5|            0|          0|                Y| 2014-04-09|2014|              PC|\n",
      "|   14631907|R27HRAA98ZYIEK|B004WYA852|     536677419|   amFilm iPad 2 Scr...|          4|            0|          0|                Y| 2012-06-29|2012|              PC|\n",
      "+-----------+--------------+----------+--------------+-----------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_limited.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1) Explore the dataset and provide analysis by product-category and year:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1) Number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------------+\n",
      "|year|    product_category|Number of Reviews|\n",
      "+----+--------------------+-----------------+\n",
      "|2014|               Books|          3540845|\n",
      "|2010|Digital_Ebook_Pur...|           102515|\n",
      "|2015|               Books|          2860727|\n",
      "|2013|            Wireless|          1767125|\n",
      "|2014|         Mobile_Apps|          1728278|\n",
      "+----+--------------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "Filtered_data.groupby(\"year\",\"product_category\").agg(F.countDistinct(\"review_id\").alias('Number of Reviews')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Number of users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------------+\n",
      "|year|    product_category|Number of Users|\n",
      "+----+--------------------+---------------+\n",
      "|2014|               Books|        1859221|\n",
      "|2010|Digital_Ebook_Pur...|          61197|\n",
      "|2015|               Books|        1548551|\n",
      "|2013|            Wireless|        1193454|\n",
      "|2014|         Mobile_Apps|         988660|\n",
      "+----+--------------------+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "Filtered_data.groupby(\"year\",\"product_category\").agg(F.countDistinct(\"customer_id\").alias('Number of Users')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Average and Median review stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+-------------+\n",
      "|year|    product_category|        Avg_Rating|Median_Rating|\n",
      "+----+--------------------+------------------+-------------+\n",
      "|2014|               Books|4.4732762187450135|            5|\n",
      "|2010|Digital_Ebook_Pur...| 3.821964043584716|            4|\n",
      "|2015|               Books| 4.497377416735384|            5|\n",
      "|2013|            Wireless| 3.820195877706306|            4|\n",
      "|2014|         Mobile_Apps|3.9685729279917052|            5|\n",
      "|2013|Digital_Video_Dow...|  4.20839463591457|            5|\n",
      "|2011|Digital_Ebook_Pur...|4.0555525408625765|            5|\n",
      "|2008|               Books| 4.233252143319705|            5|\n",
      "|2012|         Mobile_Apps|3.9950581955104916|            5|\n",
      "|2008|            Wireless|  3.76971886288676|            4|\n",
      "|2011|               Books| 4.251151283351917|            5|\n",
      "|2015|           Video_DVD|  4.52990406398113|            5|\n",
      "|2007|Digital_Video_Dow...|  3.59992298806315|            4|\n",
      "|2012|           Video_DVD| 4.217597322758286|            5|\n",
      "|2011|Digital_Video_Dow...|3.7777352349956925|            5|\n",
      "|2009|Digital_Video_Dow...| 3.699570815450644|            4|\n",
      "|2015|            Wireless| 3.985510516039572|            5|\n",
      "|2010|Digital_Video_Dow...|3.7574712643678163|            4|\n",
      "|2013|               Books|4.4125012812075175|            5|\n",
      "|2009|                  PC|3.9672266000384395|            5|\n",
      "+----+--------------------+------------------+-------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "Filtered_data.groupby(\"year\",\"product_category\").agg(F.avg(\"star_rating\").alias('Avg_Rating'),\n",
    "                                         F.expr('percentile_approx(star_rating,0.5)').alias('Median_Rating')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Percentiles of length of the review. Use the following percentiles: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[205.48028507600662, 349.16350523270023, 586.2441338149835, 853.2765772362093, 945.7590082915988, 2207.5789473684213]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length,count, mean, stddev_pop, min, max\n",
    "df1 = Filtered_data.withColumn('length', length(df.review_body))\n",
    "df2 = df1.groupby(\"year\",\"product_category\").agg(F.avg(\"length\").alias('average of star reviews'))\n",
    "colName = \"average of star reviews\"\n",
    "quantileProbs = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "relError = 0.05\n",
    "df2.stat.approxQuantile(\"average of star reviews\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)  Percentiles for number of reviews per product. For example, 10% of books got 5 or less reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 2.0, 4.0, 5108.0, 31128.0]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length,count, mean, stddev_pop, min, max\n",
    "df1 = Filtered_data.groupby(\"year\",\"product_id\",\"product_category\").agg(F.countDistinct(\"review_id\").alias('Total_Number_of_Reviews'))\n",
    "colName = \"Total_Number_of_Reviews\"\n",
    "quantileProbs = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "relError = 0.05\n",
    "df1.stat.approxQuantile(\"Total_Number_of_Reviews\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)  Identify week number (each year has 52 weeks) for each year and product category with most positive reviews (4 and 5 star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+\n",
      "|    product_category|year|week_number|\n",
      "+--------------------+----+-----------+\n",
      "|Digital_Ebook_Pur...|2014|         11|\n",
      "|Digital_Ebook_Pur...|2015|         16|\n",
      "|           Video_DVD|2015|         12|\n",
      "|           Video_DVD|2011|         37|\n",
      "|               Books|2011|         36|\n",
      "|               Books|2007|         37|\n",
      "|               Books|2008|         48|\n",
      "|Digital_Ebook_Pur...|2015|         11|\n",
      "|         Mobile_Apps|2013|          2|\n",
      "|Digital_Ebook_Pur...|2013|         49|\n",
      "|Digital_Ebook_Pur...|2013|         19|\n",
      "|               Books|2014|          6|\n",
      "|               Books|2009|         36|\n",
      "|                  PC|2010|         20|\n",
      "|               Books|2010|          3|\n",
      "|           Video_DVD|2009|          9|\n",
      "|               Books|2010|         12|\n",
      "|               Books|2006|         12|\n",
      "|                  PC|2012|         24|\n",
      "|         Mobile_Apps|2011|         32|\n",
      "+--------------------+----+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "A = Filtered_data.star_rating.isin(4)\n",
    "X = Filtered_data.star_rating.isin(5)\n",
    "df_week_number=Filtered_data.select(\"product_category\",\"year\",\"review_date\").withColumn(\"week_number\",weekofyear(\"review_date\")).where(A | X)\n",
    "df_2 = df_week_number.groupby(\"product_category\",\"year\",\"week_number\").agg(F.countDistinct(\"week_number\").alias(\"count\"))\n",
    "df_2.drop('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2) Provide detailed analysis of \"Digital eBook Purchase\" versus Books. \n",
    "\n",
    "1. Using Spark Pivot functionality, produce DataFrame with following columns:\n",
    "    1. Year\n",
    "    2. Month\n",
    "    3. Total number of reviews for \"Digital eBook Purchase\" category\n",
    "    4. Total number of reviews for \"Books\" category\n",
    "    5. Average stars for reviews for \"Digital eBook Purchase\" category\n",
    "    6. Average stars for reviews for \"Books\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+---------------------------------------------+------------------------------------------+----------------------------+-------------------------+\n",
      "|year|month(review_date)|Digital_Ebook_Purchase_total_count_of_reviews|Digital_Ebook_Purchase_Average_star_rating|Books_total_count_of_reviews|Books_Average_star_rating|\n",
      "+----+------------------+---------------------------------------------+------------------------------------------+----------------------------+-------------------------+\n",
      "|2005|                 1|                                            1|                                       5.0|                       40423|                    4.121|\n",
      "|2005|                 2|                                         null|                                      null|                       33728|                    4.125|\n",
      "|2005|                 3|                                            2|                                       4.5|                       38877|                    4.122|\n",
      "|2005|                 4|                                            1|                                       5.0|                       36886|                    4.132|\n",
      "|2005|                 5|                                            1|                                       1.0|                       36877|                    4.132|\n",
      "|2005|                 6|                                         null|                                      null|                       36607|                    4.115|\n",
      "|2005|                 7|                                            3|                                       2.0|                       45946|                    4.128|\n",
      "|2005|                 8|                                            3|                                     2.667|                       58922|                    4.186|\n",
      "|2005|                 9|                                            2|                                       4.0|                       58133|                    4.203|\n",
      "|2005|                10|                                            4|                                       4.0|                       51214|                     4.18|\n",
      "|2005|                11|                                            1|                                       5.0|                       40886|                    4.151|\n",
      "|2005|                12|                                            1|                                       5.0|                       42523|                    4.126|\n",
      "|2006|                 1|                                            8|                                     3.375|                       51996|                    4.135|\n",
      "|2006|                 2|                                            5|                                       4.6|                       54413|                    4.203|\n",
      "|2006|                 3|                                         null|                                      null|                       66896|                    4.233|\n",
      "|2006|                 4|                                         null|                                      null|                       27676|                    4.132|\n",
      "|2006|                 5|                                            1|                                       5.0|                       45011|                     4.18|\n",
      "|2006|                 6|                                            5|                                       4.2|                       48050|                    4.184|\n",
      "|2006|                 7|                                            1|                                       4.0|                       55794|                      4.2|\n",
      "|2006|                 8|                                            9|                                     4.444|                       54418|                    4.213|\n",
      "+----+------------------+---------------------------------------------+------------------------------------------+----------------------------+-------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "pivot_table = ['Digital_Ebook_Purchase','Books']\n",
    "pivot_output = Filtered_data.groupBy(\"year\",F.month(F.col(\"review_date\"))).pivot(\"product_category\",pivot_table)\\\n",
    " .agg((F.count(\"review_id\")).alias(\"total_count_of_reviews\"),\n",
    " F.round(F.mean(\"star_rating\"),3).alias(\"Average_star_rating\")).sort(\"year\",\"month(review_date)\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3)  Identify similar products (books) in both categories. Use \"product_title\" to matchproducts. To account for potential differences in naming of products, compare titlesafter stripping spaces and converting to lower case.\n",
    "\n",
    "1. Is there a difference in average rating for the similar books in digital and printed\n",
    "form?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+--------------------+-------------------------+------------------------+\n",
      "|       product_title|book_count_of_reviews|book_Avg_star_rating|       product_title|dig_book_count_of_reviews|dig_book_Avg_star_rating|\n",
      "+--------------------+---------------------+--------------------+--------------------+-------------------------+------------------------+\n",
      "|\"rays of light\": ...|                    2|                 5.0|\"rays of light\": ...|                        1|                     5.0|\n",
      "|\"the siege of khe...|                   19|               4.316|\"the siege of khe...|                      156|                   3.327|\n",
      "|          'dem bon'z|                    4|                 5.0|          'dem bon'z|                        2|                     5.0|\n",
      "|   0400 roswell time|                    1|                 5.0|   0400 roswell time|                        6|                   3.667|\n",
      "|10 smart things g...|                   19|               4.789|10 smart things g...|                        6|                   4.833|\n",
      "|10 smart things g...|                    1|                 5.0|10 smart things g...|                        6|                   4.833|\n",
      "|100 prayers for y...|                   11|                 5.0|100 prayers for y...|                        7|                     5.0|\n",
      "|13 cent killers: ...|                   37|               2.811|13 cent killers: ...|                       15|                   3.933|\n",
      "|25 essentials: te...|                   41|               4.439|25 essentials: te...|                        1|                     5.0|\n",
      "|30 before 30: tra...|                    2|                 3.5|30 before 30: tra...|                       33|                    4.97|\n",
      "|300 hard word sea...|                    2|                 4.5|300 hard word sea...|                        7|                     1.0|\n",
      "|42 rules to incre...|                    1|                 5.0|42 rules to incre...|                        2|                     5.0|\n",
      "|50 american heroe...|                    2|                 5.0|50 american heroe...|                        3|                     4.0|\n",
      "|50 successful har...|                   49|               4.347|50 successful har...|                        3|                   4.667|\n",
      "|52 prepper projec...|                   30|                 3.9|52 prepper projec...|                        2|                     4.5|\n",
      "|73 north: the bat...|                    6|                 5.0|73 north: the bat...|                        1|                     5.0|\n",
      "|<i>change</i> the...|                    8|                4.75|<i>change</i> the...|                        2|                     4.5|\n",
      "|      a changed life|                    5|                 4.2|      a changed life|                       36|                   4.222|\n",
      "|a chip off the ol...|                    1|                 5.0|a chip off the ol...|                        1|                     5.0|\n",
      "|a closer look at ...|                    1|                 5.0|a closer look at ...|                        1|                     1.0|\n",
      "+--------------------+---------------------+--------------------+--------------------+-------------------------+------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "productfilter=['Digital_Ebook_Purchase']\n",
    "digital_e_book_purchase = Filtered_data.groupBy(\"product_title\",\"product_category\")\\\n",
    " .agg((F.count(\"review_id\")).alias(\"dig_book_count_of_reviews\"),\n",
    " F.round(F.mean(\"star_rating\"),3).alias(\"dig_book_Avg_star_rating\")).filter(F.col(\"product_category\").isin(productfilter))\n",
    "\n",
    "trimmed_digital_book = digital_e_book_purchase.select(F.lower(F.trim(F.col(\"product_title\"))).alias(\"product_title\"),F.col(\"dig_book_count_of_reviews\") \\\n",
    "                      ,F.col(\"dig_book_Avg_star_rating\"))\n",
    "\n",
    "var = ['Books']\n",
    "book = Filtered_data.groupBy(\"product_title\",\"product_category\")\\\n",
    " .agg((F.count(\"review_id\")).alias(\"book_count_of_reviews\"),\n",
    " F.round(F.mean(\"star_rating\"),3).alias(\"book_Avg_star_rating\")).filter(F.col(\"product_category\").isin(var))\n",
    "\n",
    "trimmed_book = book.select(F.lower(F.trim(F.col(\"product_title\"))).alias(\"product_title\"),F.col(\"book_count_of_reviews\") \\\n",
    "                      ,F.col(\"book_Avg_star_rating\"))\n",
    "\n",
    "join_1 = trimmed_book[\"product_title\"] == trimmed_digital_book[\"product_title\"]\n",
    "joinType = \"inner\"\n",
    "final=trimmed_book.join(trimmed_digital_book, join_1, joinType)\n",
    "\n",
    "final.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The above analysis shows that Printed Books has more higher rated stars as compared to the digital form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) 3) \n",
    "2) To answer #1, you may calculate number of items with high stars in digital form\n",
    "versus printed form, and vise versa. Alternatively, you can make the conclusion by\n",
    "using appropriate pairwise statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245526"
     ]
    }
   ],
   "source": [
    "high_stars = F.col(\"book_Avg_star_rating\")>4\n",
    "final.where(high_stars).count()\n",
    "\n",
    "\n",
    "high_stars_1 = F.col(\"dig_book_Avg_star_rating\")>4\n",
    "final.where(high_stars_1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import CountVectorizer, IDF,RegexTokenizer, Tokenizer\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import struct\n",
    "import re\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2) 4)  Using provided LDA starter notebook, perform LDA topic modeling for the reviews in Digital_Ebook_Purchase and Books categories. Consider reviews for the January of 2015 only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling for reviews more than three stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = Filtered_data.filter((F.col(\"product_category\")==\"Digital_Ebook_Purchase\") | (F.col(\"product_category\")==\"Books\") \\\n",
    "                   & (F.col(\"year\")==2015) \\\n",
    "                   & (F.col(\"review_date\")<'2015-02-01')\n",
    "                   & (F.col(\"star_rating\")>3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_ml.withColumn('review_text', \n",
    "                       F.concat(F.col('review_headline'),F.lit(' '), F.col('review_body')))\n",
    "df_2 =df1.select('review_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = df_2.withColumn(\"id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = corpus_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 18287533\n",
      "+--------------------+---+\n",
      "|         review_text| id|\n",
      "+--------------------+---+\n",
      "|Nice Story but ve...|  0|\n",
      "|Beautiful and hea...|  1|\n",
      "|Worth The Wait. T...|  2|\n",
      "|written before. I...|  3|\n",
      "|Entertaining Rev....|  4|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "corpus_df.persist()\n",
    "print('Corpus size:', corpus_df.count())\n",
    "corpus_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|         review_text|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|Nice Story but ve...|[nice, story, but...|    40|\n",
      "|Beautiful and hea...|[beautiful, and, ...|    76|\n",
      "|Worth The Wait. T...|[worth, the, wait...|    77|\n",
      "|written before. I...|[written, before,...|   327|\n",
      "|Entertaining Rev....|[entertaining, re...|    51|\n",
      "|Fastest 600 page ...|[fastest, 600, pa...|    45|\n",
      "|Amazing It is a c...|[amazing, it, is,...|    27|\n",
      "|Huge impact Profo...|[huge, impact, pr...|    27|\n",
      "|LOVED LOVED LOVED...|[loved, loved, lo...|    25|\n",
      "|Five Stars very h...|[five, stars, ver...|     4|\n",
      "|This is an awesom...|[this, is, an, aw...|    26|\n",
      "|Kept me intereste...|[kept, me, intere...|    29|\n",
      "|So many of these ...|[so, many, of, th...|    50|\n",
      "|she is an incredi...|[she, is, an, inc...|    43|\n",
      "|Thoroughly enjoye...|[thoroughly, enjo...|    42|\n",
      "|This book has mad...|[this, book, has,...|    39|\n",
      "|Not as good as th...|[not, as, good, a...|    33|\n",
      "|Writer's Block Wo...|[writer, s, block...|    74|\n",
      "|One of my favorit...|[one, of, my, fav...|    43|\n",
      "|Wow This book was...|[wow, this, book,...|    74|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "'''\n",
    "tokenized_df = tokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show() \n",
    "'''\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_text\", \n",
    "                                outputCol=\"words\",pattern=\"\\\\w+\", gaps=False)\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False) pattern=\"\\\\W\"\n",
    "\n",
    "tokenized_df = regexTokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(F.col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '']\n",
    "stop_words = stop_words + ['br','book','34','m','y','ich','zu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+\n",
      "|         review_text| id|               words|            filtered|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "|Nice Story but ve...|  0|[nice, story, but...|[nice, story, rus...|\n",
      "|Beautiful and hea...|  1|[beautiful, and, ...|[beautiful, heart...|\n",
      "|Worth The Wait. T...|  2|[worth, the, wait...|[worth, wait, sto...|\n",
      "|written before. I...|  3|[written, before,...|[written, really,...|\n",
      "|Entertaining Rev....|  4|[entertaining, re...|[entertaining, re...|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|         review_text| id|               words|            filtered|       filtered_more|\n",
      "+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|Nice Story but ve...|  0|[nice, story, but...|[nice, story, rus...|[nice, story, rus...|\n",
      "|Beautiful and hea...|  1|[beautiful, and, ...|[beautiful, heart...|[beautiful, heart...|\n",
      "|Worth The Wait. T...|  2|[worth, the, wait...|[worth, wait, sto...|[worth, wait, sto...|\n",
      "|written before. I...|  3|[written, before,...|[written, really,...|[written, really,...|\n",
      "|Entertaining Rev....|  4|[entertaining, re...|[entertaining, re...|[entertaining, re...|\n",
      "+--------------------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tokenized_df1 = remover.transform(tokenized_df)\n",
    "tokenized_df1.show(5)\n",
    "\n",
    "stopwordList = stop_words\n",
    "\n",
    "remover=StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered_more\" ,stopWords=stopwordList)\n",
    "tokenized_df2 = remover.transform(tokenized_df1)\n",
    "tokenized_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+\n",
      "|       filtered_more|            features| id|\n",
      "+--------------------+--------------------+---+\n",
      "|[nice, story, rus...|(10000,[0,1,4,5,7...|  0|\n",
      "|[beautiful, heart...|(10000,[1,5,9,12,...|  1|\n",
      "|[worth, wait, sto...|(10000,[1,28,57,8...|  2|\n",
      "|[written, really,...|(10000,[0,4,5,9,1...|  3|\n",
      "|[entertaining, re...|(10000,[0,22,37,4...|  4|\n",
      "+--------------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Records in the DF: 18287533"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered_more\", outputCol=\"features\", vocabSize = 10000)\n",
    "cvmodel = cv.fit(tokenized_df2)\n",
    "featurized_df = cvmodel.transform(tokenized_df2)\n",
    "vocab = cvmodel.vocabulary\n",
    "featurized_df.select('filtered_more','features','id').show(5)\n",
    "\n",
    "countVectors = featurized_df.select('features','id')\n",
    "countVectors.persist()\n",
    "print('Records in the DF:', countVectors.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=10 means 10 words per topic\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(countVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:  0\n",
      "----------\n",
      "story\n",
      "love\n",
      "characters\n",
      "read\n",
      "series\n",
      "----------\n",
      "topic:  1\n",
      "----------\n",
      "good\n",
      "read\n",
      "story\n",
      "great\n",
      "really\n",
      "----------\n",
      "topic:  2\n",
      "----------\n",
      "read\n",
      "series\n",
      "books\n",
      "great\n",
      "love\n",
      "----------\n",
      "topic:  3\n",
      "----------\n",
      "story\n",
      "life\n",
      "read\n",
      "love\n",
      "world\n",
      "----------\n",
      "topic:  4\n",
      "----------\n",
      "read\n",
      "story\n",
      "good\n",
      "characters\n",
      "like\n",
      "----------\n",
      "topic:  5\n",
      "----------\n",
      "read\n",
      "like\n",
      "great\n",
      "time\n",
      "interesting\n",
      "----------\n",
      "topic:  6\n",
      "----------\n",
      "read\n",
      "reading\n",
      "characters\n",
      "story\n",
      "great\n",
      "----------\n",
      "topic:  7\n",
      "----------\n",
      "great\n",
      "read\n",
      "life\n",
      "good\n",
      "information\n",
      "----------\n",
      "topic:  8\n",
      "----------\n",
      "love\n",
      "story\n",
      "like\n",
      "really\n",
      "read\n",
      "----------\n",
      "topic:  9\n",
      "----------\n",
      "read\n",
      "author\n",
      "good\n",
      "books\n",
      "like\n",
      "----------"
     ]
    }
   ],
   "source": [
    "topics = model.describeTopics(5)   \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print (\"topic: \", idx)\n",
    "    print (\"----------\")\n",
    "    for word in topic:\n",
    "       print (word)\n",
    "    print (\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling for review stars less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_2 = Filtered_data.filter((F.col(\"product_category\")==\"Digital_Ebook_Purchase\") | (F.col(\"product_category\")==\"Books\") \\\n",
    "                   & (F.col(\"year\")==2015) \\\n",
    "                   & (F.col(\"review_date\")<'2015-02-01')\n",
    "                   & (F.col(\"star_rating\")<3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 17950046\n",
      "+--------------------+---+\n",
      "|         review_text| id|\n",
      "+--------------------+---+\n",
      "|Nice Story but ve...|  0|\n",
      "|Beautiful and hea...|  1|\n",
      "|Worth The Wait. T...|  2|\n",
      "|written before. I...|  3|\n",
      "|Entertaining Rev....|  4|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- id: long (nullable = false)"
     ]
    }
   ],
   "source": [
    "df1 = df_ml_2.withColumn('review_text', \n",
    "                       F.concat(F.col('review_headline'),F.lit(' '), F.col('review_body')))\n",
    "corpus =df1.select('review_text')\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "corpus_df = corpus.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "# Remove records with no review text\n",
    "corpus_df = corpus_df.dropna()\n",
    "\n",
    "corpus_df.persist()\n",
    "print('Corpus size:', corpus_df.count())\n",
    "corpus_df.show(5)\n",
    "\n",
    "corpus_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|         review_text|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|Nice Story but ve...|[nice, story, but...|    40|\n",
      "|Beautiful and hea...|[beautiful, and, ...|    76|\n",
      "|Worth The Wait. T...|[worth, the, wait...|    77|\n",
      "|written before. I...|[written, before,...|   327|\n",
      "|Entertaining Rev....|[entertaining, re...|    51|\n",
      "|Fastest 600 page ...|[fastest, 600, pa...|    45|\n",
      "|Amazing It is a c...|[amazing, it, is,...|    27|\n",
      "|Huge impact Profo...|[huge, impact, pr...|    27|\n",
      "|LOVED LOVED LOVED...|[loved, loved, lo...|    25|\n",
      "|Five Stars very h...|[five, stars, ver...|     4|\n",
      "|This is an awesom...|[this, is, an, aw...|    26|\n",
      "|Kept me intereste...|[kept, me, intere...|    29|\n",
      "|So many of these ...|[so, many, of, th...|    50|\n",
      "|she is an incredi...|[she, is, an, inc...|    43|\n",
      "|Thoroughly enjoye...|[thoroughly, enjo...|    42|\n",
      "|Not as good as th...|[not, as, good, a...|    33|\n",
      "|Writer's Block Wo...|[writer, s, block...|    74|\n",
      "|One of my favorit...|[one, of, my, fav...|    43|\n",
      "|Wow This book was...|[wow, this, book,...|    74|\n",
      "|THE BEST OF THE B...|[the, best, of, t...|    86|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "'''\n",
    "tokenized_df = tokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show() \n",
    "'''\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_text\", \n",
    "                                outputCol=\"words\",pattern=\"\\\\w+\", gaps=False)\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False) pattern=\"\\\\W\"\n",
    "\n",
    "tokenized_df = regexTokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(F.col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+\n",
      "|         review_text| id|               words|            filtered|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "|Nice Story but ve...|  0|[nice, story, but...|[nice, story, rus...|\n",
      "|Beautiful and hea...|  1|[beautiful, and, ...|[beautiful, heart...|\n",
      "|Worth The Wait. T...|  2|[worth, the, wait...|[worth, wait, sto...|\n",
      "|written before. I...|  3|[written, before,...|[written, really,...|\n",
      "|Entertaining Rev....|  4|[entertaining, re...|[entertaining, re...|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|         review_text| id|               words|            filtered|       filtered_more|\n",
      "+--------------------+---+--------------------+--------------------+--------------------+\n",
      "|Nice Story but ve...|  0|[nice, story, but...|[nice, story, rus...|[nice, story, rus...|\n",
      "|Beautiful and hea...|  1|[beautiful, and, ...|[beautiful, heart...|[beautiful, heart...|\n",
      "|Worth The Wait. T...|  2|[worth, the, wait...|[worth, wait, sto...|[worth, wait, sto...|\n",
      "|written before. I...|  3|[written, before,...|[written, really,...|[written, really,...|\n",
      "|Entertaining Rev....|  4|[entertaining, re...|[entertaining, re...|[entertaining, re...|\n",
      "+--------------------+---+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tokenized_df1 = remover.transform(tokenized_df)\n",
    "tokenized_df1.show(5)\n",
    "\n",
    "stopwordList = stop_words\n",
    "\n",
    "remover=StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered_more\" ,stopWords=stopwordList)\n",
    "tokenized_df2 = remover.transform(tokenized_df1)\n",
    "tokenized_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+\n",
      "|       filtered_more|            features| id|\n",
      "+--------------------+--------------------+---+\n",
      "|[nice, story, rus...|(10000,[0,1,4,5,7...|  0|\n",
      "|[beautiful, heart...|(10000,[1,5,9,12,...|  1|\n",
      "|[worth, wait, sto...|(10000,[1,27,56,8...|  2|\n",
      "|[written, really,...|(10000,[0,4,5,9,1...|  3|\n",
      "|[entertaining, re...|(10000,[0,22,37,4...|  4|\n",
      "+--------------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Records in the DF: 17950046"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered_more\", outputCol=\"features\", vocabSize = 10000)\n",
    "cvmodel = cv.fit(tokenized_df2)\n",
    "featurized_df = cvmodel.transform(tokenized_df2)\n",
    "vocab = cvmodel.vocabulary\n",
    "featurized_df.select('filtered_more','features','id').show(5)\n",
    "\n",
    "countVectors = featurized_df.select('features','id')\n",
    "countVectors.persist()\n",
    "print('Records in the DF:', countVectors.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in the DF: 17950046"
     ]
    }
   ],
   "source": [
    "countVectors = featurized_df.select('features','id')\n",
    "countVectors.persist()\n",
    "print('Records in the DF:', countVectors.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=10, maxIter=5)\n",
    "model = lda.fit(countVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:  0\n",
      "----------\n",
      "story\n",
      "good\n",
      "characters\n",
      "read\n",
      "love\n",
      "series\n",
      "author\n",
      "time\n",
      "like\n",
      "great\n",
      "----------\n",
      "topic:  1\n",
      "----------\n",
      "good\n",
      "read\n",
      "story\n",
      "great\n",
      "stars\n",
      "really\n",
      "like\n",
      "love\n",
      "characters\n",
      "series\n",
      "----------\n",
      "topic:  2\n",
      "----------\n",
      "read\n",
      "series\n",
      "books\n",
      "like\n",
      "great\n",
      "love\n",
      "reading\n",
      "story\n",
      "loved\n",
      "wait\n",
      "----------\n",
      "topic:  3\n",
      "----------\n",
      "story\n",
      "read\n",
      "love\n",
      "characters\n",
      "like\n",
      "written\n",
      "great\n",
      "novel\n",
      "way\n",
      "life\n",
      "----------\n",
      "topic:  4\n",
      "----------\n",
      "read\n",
      "good\n",
      "like\n",
      "great\n",
      "books\n",
      "reading\n",
      "easy\n",
      "new\n",
      "author\n",
      "people\n",
      "----------\n",
      "topic:  5\n",
      "----------\n",
      "read\n",
      "great\n",
      "time\n",
      "like\n",
      "history\n",
      "reading\n",
      "good\n",
      "life\n",
      "interesting\n",
      "stars\n",
      "----------\n",
      "topic:  6\n",
      "----------\n",
      "read\n",
      "love\n",
      "loved\n",
      "series\n",
      "characters\n",
      "story\n",
      "reading\n",
      "great\n",
      "wait\n",
      "books\n",
      "----------\n",
      "topic:  7\n",
      "----------\n",
      "great\n",
      "read\n",
      "reading\n",
      "life\n",
      "story\n",
      "recommend\n",
      "series\n",
      "god\n",
      "forward\n",
      "stars\n",
      "----------\n",
      "topic:  8\n",
      "----------\n",
      "story\n",
      "love\n",
      "really\n",
      "life\n",
      "like\n",
      "read\n",
      "characters\n",
      "loved\n",
      "stars\n",
      "know\n",
      "----------\n",
      "topic:  9\n",
      "----------\n",
      "read\n",
      "good\n",
      "author\n",
      "characters\n",
      "story\n",
      "enjoyed\n",
      "books\n",
      "like\n",
      "great\n",
      "reading\n",
      "----------"
     ]
    }
   ],
   "source": [
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print (\"topic: \", idx)\n",
    "    print (\"----------\")\n",
    "    for word in topic:\n",
    "       print (word)\n",
    "    print (\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2) 4) Does topic modeling provides good approximation to number of stars given in the review\n",
    "\n",
    "Ans : After running the LDAs for both the review stars greater than and less than 3 we can conclude that the topic modeling provides a good approximation to number of stars given in the review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
